{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current Notebook:\n",
    "1. Copy V2\n",
    "2. Hypothesis Test -- Scaling only X \n",
    "3. Hypothesis Test -- Removing Initial Zero Demand values per series "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next Notebook:\n",
    "1. Decision -- Removing Initial Zero Demand values per series \n",
    "2. Decision -- Scaling only X \n",
    "3. Hypothesis Test -- Sample Weights\n",
    "4. Hypothesis Test -- More Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_VERSION = \"05\"\n",
    "NUM_TIMESTEPS = 28\n",
    "RUN_ON_SAMPLE = False\n",
    "SAMPLE_SIZE = 500\n",
    "SCALING = False\n",
    "\n",
    "DROPOUT = 0.3\n",
    "MIN_LR = 1e-4\n",
    "MAX_LR = 1e-2\n",
    "STEP_SIZE = 2\n",
    "BATCH_SIZE = 10*1024\n",
    "PREDICT_BATCH_SIZE = 20*1024\n",
    "NUM_EPOCHS = [4, 28, 4]\n",
    "\n",
    "MC_DROPOUT = False\n",
    "MC_SAMPLES = 10 if MC_DROPOUT else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, datetime, pickle, gc\n",
    "from time import time, ctime\n",
    "from pprint import pprint\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from scipy.signal import detrend\n",
    "from scipy.stats import kurtosis, skew, median_absolute_deviation, iqr\n",
    "def signaltonoise(a, axis=0, ddof=0):\n",
    "    a = np.asanyarray(a)\n",
    "    m = a.mean(axis)\n",
    "    sd = a.std(axis=axis, ddof=ddof)\n",
    "    return np.where(sd == 0, 0, m/sd)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import Conv1D, Conv2D, LSTM, Embedding, Dense, concatenate, TimeDistributed\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout, Flatten, Reshape, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import RepeatVector, Lambda\n",
    "from tensorflow.keras.backend import repeat_elements\n",
    "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint\n",
    "\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow_addons.optimizers import TriangularCyclicalLearningRate\n",
    "from tensorflow_addons.losses import PinballLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeded_value = 88888\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "np.random.seed(seeded_value)\n",
    "tf.random.set_seed(seeded_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppress scientific notation\n",
    "pd.options.display.precision = 2\n",
    "np.set_printoptions(suppress=True)\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2.2.0']\n"
     ]
    }
   ],
   "source": [
    "print([\n",
    "    tf.__version__\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jun 26 17:13:35 2020\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "start = datetime.now()\n",
    "print(ctime(start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files\n",
    "1. calendar.csv - Contains information about the dates on which the products are sold.\n",
    "2. sales_train_validation.csv - Contains the historical daily unit sales data per product and store [d_1 - d_1913]\n",
    "3. sample_submission.csv - The correct format for submissions. Reference the Evaluation tab for more info.\n",
    "4. sell_prices.csv - Contains information about the price of the products sold per store and date.\n",
    "5. sales_train_evaluation.csv - Includes sales [d_1 - d_1941] (labels used for the Public leaderboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data/\"\n",
    "RESULTS_DIR = \"../results/\"\n",
    "PICKLE_DIR = \"../data/preprocessed/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "CARDINAL_VARS = ['state_id', 'store_id', 'cat_id', 'dept_id', 'item_id', 'id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_INDICES = np.arange(1, 1885 + 1) # Could use 1156 i.e. only 2 years of data\n",
    "VALID_INDICES = np.arange(1886, 1913 + 1)\n",
    "PUBLIC_INDICES = np.arange(1914, 1941 + 1)\n",
    "PRIVATE_INDICES = np.arange(1942, 1969 + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1885 1885\n",
      "1886 1913 28\n",
      "1914 1941 28\n",
      "1942 1969 28\n"
     ]
    }
   ],
   "source": [
    "print(min(TRAIN_INDICES), max(TRAIN_INDICES), len(TRAIN_INDICES))\n",
    "print(min(VALID_INDICES), max(VALID_INDICES), len(VALID_INDICES))\n",
    "print(min(PUBLIC_INDICES), max(PUBLIC_INDICES), len(PUBLIC_INDICES))\n",
    "print(min(PRIVATE_INDICES), max(PRIVATE_INDICES), len(PRIVATE_INDICES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read about memory management in pandas [here](https://pythonspeed.com/articles/pandas-load-less-data/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_w_first(x:pd.Series, lag:int):\n",
    "    x1 = x[:lag]\n",
    "    x2  = x.diff(lag)[lag:]\n",
    "    return pd.concat([x1, x2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(df, grouping_col, col, roll_sizes, lag_sizes):\n",
    "    \n",
    "#     # global\n",
    "#     df[col+\"_mean_g\"] = df[col].mean()\n",
    "#     df[col+\"_std_g\"] = df[col].std()\n",
    "#     df[col+\"_scaled_g\"] = (df[col] - df[col+\"_mean_g\"]) / df[col+\"_std_g\"]\n",
    "    \n",
    "#     df[col+\"_kurtosis_g\"] = kurtosis(df[col])\n",
    "#     df[col+\"_skew_g\"] = skew(df[col])\n",
    "#     df[col+\"_s2n_g\"] = signaltonoise(df[col])\n",
    "#     df[col+\"_MAD_g\"] = median_absolute_deviation(df[col])\n",
    "#     df[col+\"_iqr_g\"] = iqr(df[col])\n",
    "\n",
    "    # per series\n",
    "    df[col+\"_mean_s\"] = df.groupby(grouping_col)[col].transform(\"mean\")\n",
    "    df[col+\"_std_s\"] = df.groupby(grouping_col)[col].transform(\"std\")\n",
    "    df[col+\"_min_s\"] = df.groupby(grouping_col)[col].transform(\"min\")\n",
    "    df[col+\"_max_s\"] = df.groupby(grouping_col)[col].transform(\"max\")\n",
    "    df[col+\"_scaled_s\"] = (df[col] - df[col+\"_mean_s\"]) / df[col+\"_std_s\"]\n",
    "\n",
    "    df[col+\"_kurtosis_s\"] = df.groupby(grouping_col)[col].transform(lambda x : kurtosis(x))\n",
    "    df[col+\"_skew_s\"] = df.groupby(grouping_col)[col].transform(lambda x : skew(x))\n",
    "    df[col+\"_s2n_s\"] = df.groupby(grouping_col)[col].transform(lambda x : signaltonoise(x))\n",
    "    df[col+\"_MAD_s\"] = df.groupby(grouping_col)[col].transform(lambda x : median_absolute_deviation(x))\n",
    "    df[col+\"_iqr_s\"] = df.groupby(grouping_col)[col].transform(lambda x : iqr(x))\n",
    "\n",
    "    # rolling window per series\n",
    "    for i in roll_sizes:\n",
    "        j = str(i)\n",
    "        df[col+\"_mean_rs_\"+j] = df.groupby(grouping_col)[col].rolling(window=i, min_periods=1).mean().reset_index(drop=True)\n",
    "        df[col+\"_min_rs_\"+j] = df.groupby(grouping_col)[col].rolling(window=i, min_periods=1).min().reset_index(drop=True)\n",
    "        df[col+\"_max_rs_\"+j] = df.groupby(grouping_col)[col].rolling(window=i, min_periods=1).max().reset_index(drop=True)\n",
    "        df[col+\"_std_rs_\"+j] = df.groupby(grouping_col)[col].rolling(window=i, min_periods=1).std().reset_index(drop=True)\n",
    "        df[col+\"_std_rs_\"+j] = df[col+\"_std_rs_\"+j].fillna(1e-3)\n",
    "    \n",
    "    # various lags\n",
    "    for i in lag_sizes:\n",
    "        j = str(i)\n",
    "        df[col+\"_lag_\"+j] = df.groupby(grouping_col)[col].apply(lambda x: diff_w_first(x,i)).reset_index(drop=False)[col]\n",
    "    \n",
    "    # expanding window per series\n",
    "#     df[col+\"_mean_es\"] = df.groupby(grouping_col)[col].expanding(min_periods=1).mean().reset_index(drop=True)\n",
    "#     df[col+\"_min_es\"] = df.groupby(grouping_col)[col].expanding(min_periods=1).min().reset_index(drop=True)\n",
    "#     df[col+\"_max_es\"] = df.groupby(grouping_col)[col].expanding(min_periods=1).max().reset_index(drop=True)\n",
    "#     df[col+\"_std_es\"] = df.groupby(grouping_col)[col].expanding(min_periods=1).std().reset_index(drop=True)\n",
    "#     df[col+\"_std_es\"] = df[col+\"_std_es\"].fillna(1e-3)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(infer_mode:bool=False):\n",
    "    print('Reading files...')\n",
    "    \n",
    "    calendar = pd.read_csv('../data/calendar.csv').fillna(\"None\")\n",
    "    calendar = reduce_mem_usage(calendar)\n",
    "    print('Calendar has {} rows and {} columns'.format(calendar.shape[0], calendar.shape[1]))\n",
    "    \n",
    "    sell_prices = pd.read_csv('../data/sell_prices.csv')\n",
    "    sell_prices = reduce_mem_usage(sell_prices)\n",
    "    print('Sell prices has {} rows and {} columns'.format(sell_prices.shape[0], sell_prices.shape[1]))\n",
    "    \n",
    "    sales = pd.read_csv('../data/sales_train_evaluation.csv')\n",
    "    print('Sales train validation has {} rows and {} columns'.format(sales.shape[0], sales.shape[1]))\n",
    "    sales[['d_'+str(i) for i in PRIVATE_INDICES]] = pd.DataFrame(np.zeros(shape=(sales.shape[0], len(PRIVATE_INDICES))))\n",
    "    print('Sales train validation has {} rows and {} columns'.format(sales.shape[0], sales.shape[1]))\n",
    "    \n",
    "    submission = pd.read_csv('../data/sample_submission.csv')\n",
    "    \n",
    "    pprint({\n",
    "        \"calendar.shape\" : calendar.shape,\n",
    "        \"sell_prices.shape\" : sell_prices.shape,\n",
    "        \"sales.shape\" : sales.shape,\n",
    "        \"submission.shape\" : submission.shape\n",
    "    })\n",
    "    \n",
    "    return calendar, sell_prices, sales, submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_dateparts(calendar, datecolname):\n",
    "    calendar[\"Date\"] = pd.to_datetime(calendar[datecolname], format = (\"%Y-%m-%d\"))\n",
    "    calendar[\"Year\"] = calendar[\"Date\"].dt.year.astype('int16')\n",
    "    calendar[\"Quarter\"] = calendar[\"Date\"].dt.quarter\n",
    "    calendar[\"Month\"] = calendar[\"Date\"].dt.month\n",
    "    calendar[\"Week\"] = calendar[\"Date\"].dt.week\n",
    "    calendar[\"Day\"] = calendar[\"Date\"].dt.day\n",
    "    calendar[\"DOW\"] = calendar[\"Date\"].dt.dayofweek\n",
    "    calendar = reduce_mem_usage(calendar)\n",
    "    return calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def melt_join_fill(sales, calendar, sell_prices):\n",
    "    print(\"[INFO] \", \"Starting  -- COMPLETE\", ctime(time()))\n",
    "    if RUN_ON_SAMPLE:\n",
    "        sales = pd.melt(sales.sample(SAMPLE_SIZE),\n",
    "                        id_vars=CARDINAL_VARS,\n",
    "                        value_vars=Y_VARS,\n",
    "                        var_name=\"day_id\",\n",
    "                        value_name='demand')\n",
    "    else:\n",
    "        sales = pd.melt(sales,\n",
    "                        id_vars=CARDINAL_VARS,\n",
    "                        value_vars=Y_VARS,\n",
    "                        var_name=\"day_id\",\n",
    "                        value_name='demand')\n",
    "    print(\"[INFO] \", \"Melting  -- COMPLETE\", ctime(time()))\n",
    "\n",
    "    gc.collect()\n",
    "    \n",
    "    sales = pd.merge(sales, calendar, how=\"left\", left_on=\"day_id\", right_on=\"d\")\n",
    "    print(\"[INFO] \", \"Merging1 -- COMPLETE\", ctime(time()))\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    sales = pd.merge(sales, sell_prices, how=\"left\", on=[\"wm_yr_wk\", \"store_id\", \"item_id\"])\n",
    "    print(\"[INFO] \", \"Merging2 -- COMPLETE\", ctime(time()))\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    # if sell_price is NA\n",
    "    sales = sales.sort_values(by=CARDINAL_VARS+[\"date\"])\n",
    "    print(\"[INFO] \", \"Sorting -- COMPLETE\", ctime(time()))\n",
    "    \n",
    "    # remove till first non-zero demand per series\n",
    "    before = sales.shape\n",
    "    sales['cumul_demand'] = sales.groupby(['id'])['demand'].cumsum()\n",
    "    sales = sales.loc[sales['cumul_demand']>0].copy().reset_index(drop=True)\n",
    "    after = sales.shape\n",
    "    print(\"[INFO] \", \"Removal -- COMPLETE\", ctime(time()))\n",
    "    print(\"[INFO] \", \"Removal\", before, \"--->\", after)\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    sales[\"sell_price_available\"] = np.where(sales.sell_price.isna(), \"N\", \"Y\")\n",
    "    sales[[\"sell_price\"]] = sales.groupby([\"item_id\"])[[\"sell_price\"]].ffill()\n",
    "    sales[[\"sell_price\"]] = sales.groupby([\"item_id\"])[[\"sell_price\"]].bfill()\n",
    "    sales = sales.drop([\"d\", \"wday\", 'date', 'wm_yr_wk', 'weekday', 'month', 'year'], axis=1)\n",
    "    \n",
    "    sales['day_id'] = sales['day_id'].astype(str).apply(lambda x: x[2:]).astype(np.int16)\n",
    "    \n",
    "    print(\"[INFO] \", \"Imputing -- COMPLETE\", ctime(time()))    \n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    sales['weights'] = sales[\"sell_price\"] * sales[\"demand\"]\n",
    "    sales['rolling_weights'] = sales.groupby(CARDINAL_VARS)['weights'].rolling(window=NUM_TIMESTEPS, min_periods=1).sum().reset_index(drop=True)\n",
    "    \n",
    "    sales['weights'] = sales['weights'].apply(lambda x: np.max((1.0, x)))\n",
    "    sales['rolling_weights'] = sales['rolling_weights'].apply(lambda x: np.max((1.0, x)))\n",
    "    \n",
    "    print(\"[INFO] \", \"Weighting -- COMPLETE\", ctime(time()))    \n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    print(\"[INFO] \", \"Final dataset contains\", sales.shape)\n",
    "    \n",
    "    return sales.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickle exists hence loading from pickle file --->  ../data/preprocessed/\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(PICKLE_DIR+\"merged_df_\"+MODEL_VERSION+\".pickle\"):\n",
    "    \n",
    "    calendar, sell_prices, sales, submission = read_data()\n",
    "    \n",
    "    calendar = add_dateparts(calendar, \"date\")\n",
    "    \n",
    "    Y_VARS = sales.columns[sales.columns.str.startswith(\"d_\")]\n",
    "\n",
    "    data = melt_join_fill(sales, calendar, sell_prices)\n",
    "    \n",
    "    del sales, calendar, sell_prices\n",
    "    gc.collect()\n",
    "\n",
    "    print(\"Saving all data to ---> \", PICKLE_DIR)\n",
    "    with open(os.path.join(PICKLE_DIR, \"merged_df_\"+MODEL_VERSION+\".pickle\"),\"wb\") as f:\n",
    "        pickle.dump((data), f)\n",
    "else:\n",
    "    print(\"Pickle exists hence loading from pickle file ---> \", PICKLE_DIR)\n",
    "    with open(os.path.join(PICKLE_DIR+\"merged_df_\"+MODEL_VERSION+\".pickle\"), \"rb\") as f:\n",
    "        data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['state_id', 'store_id', 'cat_id', 'dept_id', 'item_id', 'id', 'day_id',\n",
      "       'demand', 'event_name_1', 'event_type_1', 'event_name_2',\n",
      "       'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI', 'Date', 'Year',\n",
      "       'Quarter', 'Month', 'Week', 'Day', 'DOW', 'sell_price', 'cumul_demand',\n",
      "       'sell_price_available', 'weights', 'rolling_weights', 'demand_lag',\n",
      "       'demand_lag_mean_s', 'demand_lag_std_s', 'demand_lag_min_s',\n",
      "       'demand_lag_max_s', 'demand_lag_scaled_s', 'demand_lag_kurtosis_s',\n",
      "       'demand_lag_skew_s', 'demand_lag_s2n_s', 'demand_lag_MAD_s',\n",
      "       'demand_lag_iqr_s', 'demand_lag_mean_rs_7', 'demand_lag_min_rs_7',\n",
      "       'demand_lag_max_rs_7', 'demand_lag_std_rs_7', 'demand_lag_mean_rs_30',\n",
      "       'demand_lag_min_rs_30', 'demand_lag_max_rs_30', 'demand_lag_std_rs_30',\n",
      "       'demand_lag_mean_rs_90', 'demand_lag_min_rs_90', 'demand_lag_max_rs_90',\n",
      "       'demand_lag_std_rs_90', 'demand_lag_mean_rs_365',\n",
      "       'demand_lag_min_rs_365', 'demand_lag_max_rs_365',\n",
      "       'demand_lag_std_rs_365', 'demand_lag_lag_7', 'demand_lag_lag_30',\n",
      "       'demand_lag_lag_90', 'demand_lag_lag_365'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "data[\"demand_lag\"] = data.groupby('id')['demand'].apply(lambda x: diff_w_first(x,60)).reset_index(drop=False)['demand']\n",
    "data = add_features(df=data, grouping_col=\"id\", col=\"demand_lag\", roll_sizes=[7, 30, 90, 365], lag_sizes=[7, 30, 90, 365])\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1885, 1913, 1941, 1969)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1913-28, 1913, 1913 + 28, 1913 + 28 + 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Timestamp('2011-01-29 00:00:00'), Timestamp('2016-06-19 00:00:00'))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Date.min(), data.Date.max()#, data.Date.max() + 28, data.Date.max() + 28 + 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47649940, 58)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>id</th>\n",
       "      <th>day_id</th>\n",
       "      <th>demand</th>\n",
       "      <th>event_name_1</th>\n",
       "      <th>event_type_1</th>\n",
       "      <th>...</th>\n",
       "      <th>demand_lag_max_rs_90</th>\n",
       "      <th>demand_lag_std_rs_90</th>\n",
       "      <th>demand_lag_mean_rs_365</th>\n",
       "      <th>demand_lag_min_rs_365</th>\n",
       "      <th>demand_lag_max_rs_365</th>\n",
       "      <th>demand_lag_std_rs_365</th>\n",
       "      <th>demand_lag_lag_7</th>\n",
       "      <th>demand_lag_lag_30</th>\n",
       "      <th>demand_lag_lag_90</th>\n",
       "      <th>demand_lag_lag_365</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CA</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>FOODS_1</td>\n",
       "      <td>FOODS_1_001</td>\n",
       "      <td>FOODS_1_001_CA_1_evaluation</td>\n",
       "      <td>1</td>\n",
       "      <td>3.00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CA</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>FOODS_1</td>\n",
       "      <td>FOODS_1_001</td>\n",
       "      <td>FOODS_1_001_CA_1_evaluation</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>3.00</td>\n",
       "      <td>2.12</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>2.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CA</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>FOODS_1</td>\n",
       "      <td>FOODS_1_001</td>\n",
       "      <td>FOODS_1_001_CA_1_evaluation</td>\n",
       "      <td>3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1.73</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1.73</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CA</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>FOODS_1</td>\n",
       "      <td>FOODS_1_001</td>\n",
       "      <td>FOODS_1_001_CA_1_evaluation</td>\n",
       "      <td>4</td>\n",
       "      <td>1.00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1.41</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1.41</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CA</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>FOODS_1</td>\n",
       "      <td>FOODS_1_001</td>\n",
       "      <td>FOODS_1_001_CA_1_evaluation</td>\n",
       "      <td>5</td>\n",
       "      <td>4.00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.82</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  state_id store_id cat_id  dept_id      item_id                           id  \\\n",
       "0       CA     CA_1  FOODS  FOODS_1  FOODS_1_001  FOODS_1_001_CA_1_evaluation   \n",
       "1       CA     CA_1  FOODS  FOODS_1  FOODS_1_001  FOODS_1_001_CA_1_evaluation   \n",
       "2       CA     CA_1  FOODS  FOODS_1  FOODS_1_001  FOODS_1_001_CA_1_evaluation   \n",
       "3       CA     CA_1  FOODS  FOODS_1  FOODS_1_001  FOODS_1_001_CA_1_evaluation   \n",
       "4       CA     CA_1  FOODS  FOODS_1  FOODS_1_001  FOODS_1_001_CA_1_evaluation   \n",
       "\n",
       "   day_id  demand event_name_1 event_type_1  ... demand_lag_max_rs_90  \\\n",
       "0       1    3.00         None         None  ...                 3.00   \n",
       "1       2    0.00         None         None  ...                 3.00   \n",
       "2       3    0.00         None         None  ...                 3.00   \n",
       "3       4    1.00         None         None  ...                 3.00   \n",
       "4       5    4.00         None         None  ...                 4.00   \n",
       "\n",
       "  demand_lag_std_rs_90  demand_lag_mean_rs_365  demand_lag_min_rs_365  \\\n",
       "0                 0.00                    3.00                   3.00   \n",
       "1                 2.12                    1.50                   0.00   \n",
       "2                 1.73                    1.00                   0.00   \n",
       "3                 1.41                    1.00                   0.00   \n",
       "4                 1.82                    1.60                   0.00   \n",
       "\n",
       "   demand_lag_max_rs_365 demand_lag_std_rs_365  demand_lag_lag_7  \\\n",
       "0                   3.00                  0.00              3.00   \n",
       "1                   3.00                  2.12              0.00   \n",
       "2                   3.00                  1.73              0.00   \n",
       "3                   3.00                  1.41              1.00   \n",
       "4                   4.00                  1.82              4.00   \n",
       "\n",
       "   demand_lag_lag_30  demand_lag_lag_90  demand_lag_lag_365  \n",
       "0               3.00               3.00                3.00  \n",
       "1               0.00               0.00                0.00  \n",
       "2               0.00               0.00                0.00  \n",
       "3               1.00               1.00                1.00  \n",
       "4               4.00               4.00                4.00  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data.shape); data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONT_VARS = ['snap_CA', 'snap_TX', 'snap_WI', 'sell_price',\n",
    "             'demand_lag', 'demand_lag_mean_s', 'demand_lag_std_s', 'demand_lag_min_s',\n",
    "             'demand_lag_max_s', 'demand_lag_scaled_s', 'demand_lag_kurtosis_s',\n",
    "             'demand_lag_skew_s', 'demand_lag_s2n_s', 'demand_lag_MAD_s',\n",
    "             'demand_lag_iqr_s', 'demand_lag_mean_rs_7', 'demand_lag_min_rs_7',\n",
    "             'demand_lag_max_rs_7', 'demand_lag_std_rs_7', 'demand_lag_mean_rs_30',\n",
    "             'demand_lag_min_rs_30', 'demand_lag_max_rs_30', 'demand_lag_std_rs_30',\n",
    "             'demand_lag_mean_rs_90', 'demand_lag_min_rs_90', 'demand_lag_max_rs_90',\n",
    "             'demand_lag_std_rs_90', 'demand_lag_mean_rs_365',\n",
    "             'demand_lag_min_rs_365', 'demand_lag_max_rs_365',\n",
    "             'demand_lag_std_rs_365', 'demand_lag_lag_7', 'demand_lag_lag_30',\n",
    "             'demand_lag_lag_90', 'demand_lag_lag_365']\n",
    "CAT_VARS = ['state_id', 'store_id', 'cat_id', 'dept_id', 'item_id', 'event_name_1',\n",
    "            'event_type_1', 'event_name_2', 'event_type_2', 'Year', 'Quarter',\n",
    "            'Month', 'Week', 'Day', 'DOW', 'sell_price_available']\n",
    "DEP_VAR = ['demand']\n",
    "WEIGHT_VAR = ['weights', 'rolling_weights']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Category_Mapping = {}\n",
    "for c in CAT_VARS:\n",
    "    # convert columns to categories\n",
    "    data[c+\"_cat\"] = data[c].astype(\"category\")\n",
    "    \n",
    "    # save the mapping for later use\n",
    "    Category_Mapping.update({c+\"_cat\" : dict(enumerate(data[c+\"_cat\"].cat.categories))})\n",
    "    \n",
    "    # Copy categories as integer codes\n",
    "    data[c+\"_cat\"] = data[c+\"_cat\"].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DOW_cat': {'emb_sz': 3, 'max': 6, 'min': 0, 'nuniq': 7},\n",
      " 'Day_cat': {'emb_sz': 10, 'max': 30, 'min': 0, 'nuniq': 31},\n",
      " 'Month_cat': {'emb_sz': 6, 'max': 11, 'min': 0, 'nuniq': 12},\n",
      " 'Quarter_cat': {'emb_sz': 2, 'max': 3, 'min': 0, 'nuniq': 4},\n",
      " 'Week_cat': {'emb_sz': 10, 'max': 52, 'min': 0, 'nuniq': 53},\n",
      " 'Year_cat': {'emb_sz': 3, 'max': 5, 'min': 0, 'nuniq': 6},\n",
      " 'cat_id_cat': {'emb_sz': 2, 'max': 2, 'min': 0, 'nuniq': 3},\n",
      " 'dept_id_cat': {'emb_sz': 3, 'max': 6, 'min': 0, 'nuniq': 7},\n",
      " 'event_name_1_cat': {'emb_sz': 10, 'max': 30, 'min': 0, 'nuniq': 31},\n",
      " 'event_name_2_cat': {'emb_sz': 2, 'max': 4, 'min': 0, 'nuniq': 5},\n",
      " 'event_type_1_cat': {'emb_sz': 2, 'max': 4, 'min': 0, 'nuniq': 5},\n",
      " 'event_type_2_cat': {'emb_sz': 2, 'max': 2, 'min': 0, 'nuniq': 3},\n",
      " 'item_id_cat': {'emb_sz': 10, 'max': 3048, 'min': 0, 'nuniq': 3049},\n",
      " 'sell_price_available_cat': {'emb_sz': 2, 'max': 0, 'min': 0, 'nuniq': 1},\n",
      " 'state_id_cat': {'emb_sz': 2, 'max': 2, 'min': 0, 'nuniq': 3},\n",
      " 'store_id_cat': {'emb_sz': 5, 'max': 9, 'min': 0, 'nuniq': 10}}\n"
     ]
    }
   ],
   "source": [
    "Unique_Dict ={}\n",
    "for c in CAT_VARS:\n",
    "    col = c+\"_cat\"\n",
    "    Unique_Dict.update({col:{'min':data[col].min(), \n",
    "                             'max':data[col].max(), \n",
    "                             'nuniq':data[col].nunique(), \n",
    "                             'emb_sz':max(min(int(data[col].nunique() / 2), 10), 2)}})\n",
    "pprint(Unique_Dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_CAT_VARS = [col+\"_cat\" for col in CAT_VARS]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MMS = MinMaxScaler()\n",
    "data[CONT_VARS] = MMS.fit_transform(data[CONT_VARS])\n",
    "\n",
    "if SCALING:\n",
    "\n",
    "    MMS_Y = MinMaxScaler(feature_range=(0, 0.8))\n",
    "    data[DEP_VAR] = MMS_Y.fit_transform(data[DEP_VAR])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['set'] = np.where(data.day_id.isin(TRAIN_INDICES), \"Train\",\n",
    "                       np.where(data.day_id.isin(VALID_INDICES), \"Valid\",\n",
    "                                np.where(data.day_id.isin(PUBLIC_INDICES), \"Public\", \"Private\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby('set').agg({\n",
    "    'id':'count',\n",
    "    'day_id':[np.min, np.max]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data.loc[data.day_id.isin(TRAIN_INDICES)]\n",
    "valid_data = data.loc[data.day_id.isin(VALID_INDICES)]\n",
    "public_data = data.loc[data.day_id.isin(PUBLIC_INDICES)]\n",
    "private_data = data.loc[data.day_id.isin(PRIVATE_INDICES)]\n",
    "train_data.shape, valid_data.shape, public_data.shape, private_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape[0] == train_data.shape[0] + valid_data.shape[0] + private_data.shape[0] + private_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_time = time()\n",
    "model_t = datetime.now()\n",
    "print(ctime(model_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    layers = []\n",
    "    inputs = []\n",
    "    for i,col in enumerate(CAT_VARS):\n",
    "        input_ = Input(shape=1, name=col+\"_cat\")\n",
    "        embedding =  Embedding(Unique_Dict[col+\"_cat\"]['nuniq'],\n",
    "                               Unique_Dict[col+\"_cat\"]['emb_sz'],\n",
    "                               name='emb_'+col)(input_)\n",
    "        vec = Flatten()(embedding)\n",
    "        layers.append(vec)\n",
    "        inputs.append(input_)\n",
    "    \n",
    "    for i, col in enumerate(CONT_VARS):\n",
    "        input_ = Input(shape=1, name=col)\n",
    "        layers.append(input_)\n",
    "        inputs.append(input_)\n",
    "    \n",
    "    concat_layer = concatenate(layers)\n",
    "    x = Dense(2048)(concat_layer)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(DROPOUT)(x, training=MC_DROPOUT)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Dense(1024)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(DROPOUT)(x, training=MC_DROPOUT)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Dense(512)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(DROPOUT)(x, training=MC_DROPOUT)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    x = Dense(256)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(DROPOUT)(x, training=MC_DROPOUT)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    x = Dense(128)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(DROPOUT)(x, training=MC_DROPOUT)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    x = Dense(64)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(DROPOUT)(x, training=MC_DROPOUT)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    x = Dense(32)(x)\n",
    "    x = Dropout(DROPOUT)(x, training=MC_DROPOUT)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    if SCALING:\n",
    "        demand = Dense(1, activation='sigmoid', name='demand')(x)\n",
    "    else:\n",
    "        demand = Dense(1, activation='relu', name='demand')(x)\n",
    "    \n",
    "    model = Model(inputs, demand)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(RESULTS_DIR+\"BestCheckpoint_\"+MODEL_VERSION+\".h5\"):\n",
    "    model.load_weights(RESULTS_DIR+\"BestCheckpoint_\"+MODEL_VERSION+\".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tclr = TriangularCyclicalLearningRate(\n",
    "    initial_learning_rate=MIN_LR,\n",
    "    maximal_learning_rate=MAX_LR,\n",
    "    step_size=STEP_SIZE * len(train_data)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcp = ModelCheckpoint(filepath=RESULTS_DIR+\"BestCheckpoint_\"+MODEL_VERSION+\".h5\", monitor='val_loss',\n",
    "                      verbose=0, save_best_only=True, save_weights_only=False, mode='min', save_freq='epoch')\n",
    "csvl = CSVLogger(filename=RESULTS_DIR+\"LossLogs_\"+MODEL_VERSION+\".csv\",\n",
    "                 separator=\",\", append=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PBL = PinballLoss(tau=[0.005,0.025,0.165,0.25,0.5,0.75,0.835,0.975,0.995])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = Adam(learning_rate=MAX_LR)\n",
    "model.compile(loss= PBL if MC_DROPOUT else 'mse', optimizer=adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(x=train_data[NEW_CAT_VARS + CONT_VARS].to_dict(orient='series'),\n",
    "                    y=train_data[DEP_VAR].to_dict(orient='series'),\n",
    "                    validation_data=(valid_data[NEW_CAT_VARS + CONT_VARS].to_dict(orient='series'),\n",
    "                                     valid_data[DEP_VAR].to_dict(orient='series'),\n",
    "                                     valid_data[WEIGHT_VAR].to_dict(orient='series')['rolling_weights']\n",
    "                                    ),\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=NUM_EPOCHS[0],\n",
    "                    shuffle=True,\n",
    "                    verbose=1,\n",
    "                    sample_weight=train_data[WEIGHT_VAR].to_dict(orient='series')['rolling_weights'],\n",
    "                    callbacks=[mcp, csvl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_names = ['loss']\n",
    "\n",
    "for i, j in zip(metric_names, ['val_'+i for i in metric_names]):\n",
    "    plt.plot(history.history[i])\n",
    "    plt.plot(history.history[j])\n",
    "    plt.title('Model '+i)\n",
    "    plt.ylabel(i.upper())\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(filepath=RESULTS_DIR+\"BestCheckpoint_\"+MODEL_VERSION+\".h5\", compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = Adam(learning_rate=tclr)\n",
    "model.compile(loss=PBL if MC_DROPOUT else 'mse', optimizer=adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(x=train_data[NEW_CAT_VARS + CONT_VARS].to_dict(orient='series'),\n",
    "                    y=train_data[DEP_VAR].to_dict(orient='series'),\n",
    "                    validation_data=(valid_data[NEW_CAT_VARS + CONT_VARS].to_dict(orient='series'),\n",
    "                                     valid_data[DEP_VAR].to_dict(orient='series'),\n",
    "                                     valid_data[WEIGHT_VAR].to_dict(orient='series')['rolling_weights']\n",
    "                                    ),\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=NUM_EPOCHS[1],\n",
    "                    shuffle=True,\n",
    "                    verbose=1, \n",
    "                    sample_weight=train_data[WEIGHT_VAR].to_dict(orient='series')['rolling_weights'],\n",
    "                    callbacks=[mcp, csvl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_names = ['loss']\n",
    "\n",
    "for i, j in zip(metric_names, ['val_'+i for i in metric_names]):\n",
    "    plt.plot(history.history[i])\n",
    "    plt.plot(history.history[j])\n",
    "    plt.title('Model ' + i)\n",
    "    plt.ylabel(i.upper())\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(filepath=RESULTS_DIR+\"BestCheckpoint_\"+MODEL_VERSION+\".h5\", compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = Adam(learning_rate=MIN_LR*0.1)\n",
    "model.compile(loss=PBL if MC_DROPOUT else 'mse', optimizer=adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history = model.fit(x=train_data[NEW_CAT_VARS + CONT_VARS].to_dict(orient='series'),\n",
    "                    y=train_data[DEP_VAR].to_dict(orient='series'),\n",
    "                    validation_data=(valid_data[NEW_CAT_VARS + CONT_VARS].to_dict(orient='series'),\n",
    "                                     valid_data[DEP_VAR].to_dict(orient='series'),\n",
    "                                     valid_data[WEIGHT_VAR].to_dict(orient='series')['rolling_weights']\n",
    "                                    ),\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=NUM_EPOCHS[2],\n",
    "                    shuffle=True,\n",
    "                    verbose=1, \n",
    "                    sample_weight=train_data[WEIGHT_VAR].to_dict(orient='series')['rolling_weights'],\n",
    "                    callbacks=[mcp, csvl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metric_names = ['loss']\n",
    "\n",
    "for i, j in zip(metric_names, ['val_'+i for i in metric_names]):\n",
    "    plt.plot(history.history[i])\n",
    "    plt.plot(history.history[j])\n",
    "    plt.title('Model '+i)\n",
    "    plt.ylabel(i.upper())\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(filepath=RESULTS_DIR+\"FinalCheckpoint_\"+MODEL_VERSION+\".h5\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(RESULTS_DIR+\"BestCheckpoint_\"+MODEL_VERSION+\".h5\"):\n",
    "    model = tf.keras.models.load_model(filepath=RESULTS_DIR+\"BestCheckpoint_\"+MODEL_VERSION+\".h5\", compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(MC_SAMPLES):\n",
    "    if i == 0:\n",
    "        train_preds = model.predict(train_data[NEW_CAT_VARS + CONT_VARS].to_dict(orient='series'),\n",
    "                                    batch_size=PREDICT_BATCH_SIZE, verbose=0)\n",
    "\n",
    "        valid_preds = model.predict(valid_data[NEW_CAT_VARS + CONT_VARS].to_dict(orient='series'),\n",
    "                                    batch_size=PREDICT_BATCH_SIZE, verbose=0)\n",
    "\n",
    "        public_preds = model.predict(public_data[NEW_CAT_VARS + CONT_VARS].to_dict(orient='series'),\n",
    "                                     batch_size=PREDICT_BATCH_SIZE, verbose=0)\n",
    "\n",
    "        private_preds = model.predict(private_data[NEW_CAT_VARS + CONT_VARS].to_dict(orient='series'),\n",
    "                                      batch_size=PREDICT_BATCH_SIZE, verbose=0)\n",
    "        \n",
    "    else:\n",
    "        train_preds = np.concatenate((train_preds, \n",
    "                                      model.predict(train_data[NEW_CAT_VARS + CONT_VARS].to_dict(orient='series'),\n",
    "                                                    batch_size=PREDICT_BATCH_SIZE, verbose=0)), axis=1)\n",
    "\n",
    "        valid_preds = np.concatenate((valid_preds, \n",
    "                                      model.predict(valid_data[NEW_CAT_VARS + CONT_VARS].to_dict(orient='series'),\n",
    "                                                    batch_size=PREDICT_BATCH_SIZE, verbose=0)), axis=1)\n",
    "\n",
    "        public_preds = np.concatenate((public_preds, \n",
    "                                       model.predict(public_data[NEW_CAT_VARS + CONT_VARS].to_dict(orient='series'),\n",
    "                                                     batch_size=PREDICT_BATCH_SIZE, verbose=0)), axis=1)\n",
    "\n",
    "        private_preds = np.concatenate((private_preds, \n",
    "                                        model.predict(private_data[NEW_CAT_VARS + CONT_VARS].to_dict(orient='series'),\n",
    "                                                      batch_size=PREDICT_BATCH_SIZE, verbose=0)), axis=1)\n",
    "\n",
    "print(train_preds.shape, valid_preds.shape, public_preds.shape, private_preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SCALING:\n",
    "    train_demand = MMS_Y.inverse_transform(train_preds)\n",
    "    valid_demand = MMS_Y.inverse_transform(valid_preds)\n",
    "    public_demand = MMS_Y.inverse_transform(public_preds)\n",
    "    private_demand = MMS_Y.inverse_transform(private_preds)\n",
    "    \n",
    "    data['prediction'] = 0\n",
    "    data[['prediction']].loc[data.day_id.isin(TRAIN_INDICES)] = train_preds\n",
    "    data[['prediction']].loc[data.day_id.isin(VALID_INDICES)] = valid_preds\n",
    "    data[['prediction']].loc[data.day_id.isin(PUBLIC_INDICES)] = public_preds\n",
    "    data[['prediction']].loc[data.day_id.isin(PRIVATE_INDICES)] = private_preds\n",
    "\n",
    "    data['demand_Unscaled'] = MMS_Y.inverse_transform(data[['demand']])\n",
    "    data[[col+'_Unscaled' for col in CONT_VARS]] = pd.DataFrame(MMS.inverse_transform(data[CONT_VARS]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['pred_demand'] = 0\n",
    "\n",
    "if SCALING:\n",
    "    data['pred_demand'].loc[data.day_id.isin(TRAIN_INDICES)] = train_demand.flatten()\n",
    "    data['pred_demand'].loc[data.day_id.isin(VALID_INDICES)] = valid_demand.flatten()\n",
    "    data['pred_demand'].loc[data.day_id.isin(PUBLIC_INDICES)] = public_demand.flatten()\n",
    "    data['pred_demand'].loc[data.day_id.isin(PRIVATE_INDICES)] = private_demand.flatten()\n",
    "else:\n",
    "    data['pred_demand'].loc[data.day_id.isin(TRAIN_INDICES)] = train_preds.flatten()\n",
    "    data['pred_demand'].loc[data.day_id.isin(VALID_INDICES)] = valid_preds.flatten()\n",
    "    data['pred_demand'].loc[data.day_id.isin(PUBLIC_INDICES)] = public_preds.flatten()\n",
    "    data['pred_demand'].loc[data.day_id.isin(PRIVATE_INDICES)] = private_preds.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ['Train', 'Valid', 'Public', 'Private']:\n",
    "    print(i)\n",
    "    print('\\t',{\n",
    "        i+'_RMSE' : np.round(mean_squared_error(y_true = data['demand'].loc[data.set == i],\n",
    "                                                y_pred = data['pred_demand'].loc[data.set == i]),\n",
    "                             2) if i!='Private' else 'None',\n",
    "        i+\"_REAL_SUM\" : np.round(data['demand'].loc[data.set != i].sum(), 2),\n",
    "        i+\"_PRED_SUM\" : np.round(data['pred_demand'].loc[data.set != i].sum(), 2),\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is the distribution same?\n",
    "data[['demand','pred_demand','set']].groupby('set').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = data.loc[data['set'].isin(['Private', 'Public'])][['id',\n",
    "                                                                'day_id',\n",
    "                                                                'pred_demand', \n",
    "                                                                'set']].copy().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['day_id2'] = 'F' + pd.Series(np.where(submission['day_id'] <= max(PUBLIC_INDICES), \n",
    "                                                 submission['day_id'] - min(PUBLIC_INDICES) + 1, \n",
    "                                                 submission['day_id'] - min(PRIVATE_INDICES) + 1).astype(int).astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['id'] = np.where(submission['set']==\"Public\",\n",
    "                            submission['id'].str.replace(\"evaluation\", \"validation\"), \n",
    "                            submission['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.groupby('set').agg({\n",
    "    'day_id':[np.min, np.max]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_file = submission.pivot(\n",
    "    values='pred_demand',\n",
    "    index='id',\n",
    "    columns='day_id2').reset_index(drop=False)\n",
    "submission_file = submission_file[['id']+[\"F\"+str(i) for i in range(1,29,1)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_file.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_file.to_csv('../results/submission_'+MODEL_VERSION+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time()\n",
    "end = datetime.now()\n",
    "print(ctime(end_time))\n",
    "print(end - start, model_t - start, end - model_t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_tensorflow",
   "language": "python",
   "name": "conda-env-py37_tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
