{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_VERSION = \"03\"\n",
    "NUM_TIMESTEPS = 28\n",
    "RUN_ON_SAMPLE = True\n",
    "SAMPLE_SIZE = 500\n",
    "SCALING = False\n",
    "\n",
    "DROPOUT = 0.3\n",
    "MIN_LR = 1e-4\n",
    "MAX_LR = 1e-2\n",
    "STEP_SIZE = 2\n",
    "BATCH_SIZE = 2*1024\n",
    "PREDICT_BATCH_SIZE = 4*1024\n",
    "NUM_EPOCHS = [2, 2, 2]\n",
    "\n",
    "MC_DROPOUT = True\n",
    "MC_SAMPLES = 100 if MC_DROPOUT else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, datetime, pickle, gc\n",
    "from time import time, ctime\n",
    "from pprint import pprint\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import Conv1D, Conv2D, LSTM, Embedding, Dense, concatenate, TimeDistributed\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout, Flatten, Reshape, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import RepeatVector, Lambda\n",
    "from tensorflow.keras.backend import repeat_elements\n",
    "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint\n",
    "\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow_addons.optimizers import TriangularCyclicalLearningRate\n",
    "from tensorflow_addons.losses import PinballLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeded_value = 88888\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "np.random.seed(seeded_value)\n",
    "tf.random.set_seed(seeded_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppress scientific notation\n",
    "pd.options.display.precision = 2\n",
    "np.set_printoptions(suppress=True)\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2.1.0']\n"
     ]
    }
   ],
   "source": [
    "print([\n",
    "    tf.__version__\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jun 28 04:48:28 2020\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "start = datetime.now()\n",
    "print(ctime(start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files\n",
    "1. calendar.csv - Contains information about the dates on which the products are sold.\n",
    "2. sales_train_validation.csv - Contains the historical daily unit sales data per product and store [d_1 - d_1913]\n",
    "3. sample_submission.csv - The correct format for submissions. Reference the Evaluation tab for more info.\n",
    "4. sell_prices.csv - Contains information about the price of the products sold per store and date.\n",
    "5. sales_train_evaluation.csv - Includes sales [d_1 - d_1941] (labels used for the Public leaderboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data/\"\n",
    "RESULTS_DIR = \"../results/\"\n",
    "PICKLE_DIR = \"../data/preprocessed/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "CARDINAL_VARS = ['state_id', 'store_id', 'cat_id', 'dept_id', 'item_id', 'id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_INDICES = np.arange(1, 1885 + 1) # Could use 1156 i.e. only 2 years of data\n",
    "VALID_INDICES = np.arange(1886, 1913 + 1)\n",
    "PUBLIC_INDICES = np.arange(1914, 1941 + 1)\n",
    "PRIVATE_INDICES = np.arange(1942, 1969 + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1885 1885\n",
      "1886 1913 28\n",
      "1914 1941 28\n",
      "1942 1969 28\n"
     ]
    }
   ],
   "source": [
    "print(min(TRAIN_INDICES), max(TRAIN_INDICES), len(TRAIN_INDICES))\n",
    "print(min(VALID_INDICES), max(VALID_INDICES), len(VALID_INDICES))\n",
    "print(min(PUBLIC_INDICES), max(PUBLIC_INDICES), len(PUBLIC_INDICES))\n",
    "print(min(PRIVATE_INDICES), max(PRIVATE_INDICES), len(PRIVATE_INDICES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read about memory management in pandas [here](https://pythonspeed.com/articles/pandas-load-less-data/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_w_first(x:pd.Series, lag:int):\n",
    "    x1 = x[:lag]\n",
    "    x2  = x.diff(lag)[lag:]\n",
    "    return pd.concat([x1, x2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(df, grouping_col, col, roll_sizes, lag_sizes):\n",
    "    \n",
    "    # per series\n",
    "    df[col+\"_mean_s\"] = df.groupby(grouping_col)[col].transform(\"mean\")\n",
    "    df[col+\"_std_s\"] = df.groupby(grouping_col)[col].transform(\"std\")\n",
    "    df[col+\"_min_s\"] = df.groupby(grouping_col)[col].transform(\"min\")\n",
    "    df[col+\"_max_s\"] = df.groupby(grouping_col)[col].transform(\"max\")\n",
    "    df[col+\"_scaled_s\"] = (df[col] - df[col+\"_mean_s\"]) / df[col+\"_std_s\"]\n",
    "    \n",
    "    # rolling window per series\n",
    "    for i in roll_sizes:\n",
    "        j = str(i)\n",
    "        df[col+\"_mean_rs_\"+j] = df.groupby(grouping_col)[col].rolling(window=i, min_periods=1).mean().reset_index(drop=True)\n",
    "        df[col+\"_min_rs_\"+j] = df.groupby(grouping_col)[col].rolling(window=i, min_periods=1).min().reset_index(drop=True)\n",
    "        df[col+\"_max_rs_\"+j] = df.groupby(grouping_col)[col].rolling(window=i, min_periods=1).max().reset_index(drop=True)\n",
    "        df[col+\"_std_rs_\"+j] = df.groupby(grouping_col)[col].rolling(window=i, min_periods=1).std().reset_index(drop=True)\n",
    "        df[col+\"_std_rs_\"+j] = df[col+\"_std_rs_\"+j].fillna(1e-3)\n",
    "    \n",
    "    # various lags\n",
    "    for i in lag_sizes:\n",
    "        j = str(i)\n",
    "        df[col+\"_lag_\"+j] = df.groupby(grouping_col)[col].apply(lambda x: diff_w_first(x,i)).reset_index(drop=False)[col]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_id_after_aggregations(df):\n",
    "    subs = pd.read_csv('../data/sample_submission_uncertainty.csv')\n",
    "    subs['id_interim'] = subs['id'].str.split(\"_\").apply(lambda x: \"_\".join(x[:-2]).replace(\"_X\",\"\"))\n",
    "    subs['id_trimmed'] = subs.id.str.split(\"_\").apply(lambda x: \"_\".join(x[:-2]))\n",
    "\n",
    "    df['id_interim'] = ''\n",
    "    df['id_interim'].loc[(~df['item_id'].isna()) & (~df['store_id'].isna())] = df[['item_id', 'store_id']].fillna(\"\").apply(lambda x: \"_\".join([i for i in x if i!=\"\"]), axis=1)\n",
    "    df['id_interim'].loc[(df['item_id'].isna()) | (df['store_id'].isna())] = df[['state_id', 'store_id', 'cat_id', 'dept_id', 'item_id']].fillna(\"\").apply(lambda x: \"_\".join([i for i in x if i!=\"\"]), axis=1)\n",
    "    df['id_interim'].loc[df['id_interim']==''] = 'Total'\n",
    "\n",
    "    df2 = df.merge(subs.loc[(subs.id.str.endswith('_evaluation')) & (subs.id.str.contains('0.005'))][['id_trimmed','id_interim']],\n",
    "                   how='left', on='id_interim')\n",
    "\n",
    "    df2.rename(columns={'id_trimmed':'id'}, inplace=True)\n",
    "    df2.drop('id_interim', axis=1, inplace=True)\n",
    "    return df2[['id','dept_id','cat_id','store_id','state_id','item_id'] + ['d_'+str(i) for i in range(1,1942)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(infer_mode:bool=False):\n",
    "    print('Reading files...')\n",
    "    \n",
    "    calendar = pd.read_csv('../data/calendar.csv').fillna(\"None\")\n",
    "    calendar = reduce_mem_usage(calendar)\n",
    "    print('Calendar has {} rows and {} columns'.format(calendar.shape[0], calendar.shape[1]))\n",
    "    \n",
    "    sell_prices = pd.read_csv('../data/sell_prices.csv')\n",
    "    sell_prices = reduce_mem_usage(sell_prices)\n",
    "    print('Sell prices has {} rows and {} columns'.format(sell_prices.shape[0], sell_prices.shape[1]))\n",
    "    \n",
    "    sales = pd.read_csv('../data/sales_train_evaluation_with_aggregations.csv')\n",
    "    sales = add_id_after_aggregations(df=sales)\n",
    "    print('Sales train validation has {} rows and {} columns'.format(sales.shape[0], sales.shape[1]))\n",
    "    sales[['d_'+str(i) for i in PRIVATE_INDICES]] = pd.DataFrame(np.zeros(shape=(sales.shape[0], len(PRIVATE_INDICES))))\n",
    "    print('Sales train validation has {} rows and {} columns'.format(sales.shape[0], sales.shape[1]))\n",
    "    \n",
    "    submission = pd.read_csv('../data/sample_submission.csv')\n",
    "    \n",
    "    pprint({\n",
    "        \"calendar.shape\" : calendar.shape,\n",
    "        \"sell_prices.shape\" : sell_prices.shape,\n",
    "        \"sales.shape\" : sales.shape,\n",
    "        \"submission.shape\" : submission.shape\n",
    "    })\n",
    "    \n",
    "    return calendar, sell_prices, sales, submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_dateparts(calendar, datecolname):\n",
    "    calendar[\"Date\"] = pd.to_datetime(calendar[datecolname], format = (\"%Y-%m-%d\"))\n",
    "    calendar[\"Year\"] = calendar[\"Date\"].dt.year.astype('int16')\n",
    "    calendar[\"Quarter\"] = calendar[\"Date\"].dt.quarter\n",
    "    calendar[\"Month\"] = calendar[\"Date\"].dt.month\n",
    "    calendar[\"Week\"] = calendar[\"Date\"].dt.week\n",
    "    calendar[\"Day\"] = calendar[\"Date\"].dt.day\n",
    "    calendar[\"DOW\"] = calendar[\"Date\"].dt.dayofweek\n",
    "    calendar = reduce_mem_usage(calendar)\n",
    "    return calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def melt_join_fill(sales, calendar, sell_prices):\n",
    "    \n",
    "    print(\"[INFO] \", \"Starting  -- COMPLETE\", ctime(time()))\n",
    "    if RUN_ON_SAMPLE:\n",
    "        sales = pd.melt(sales.sample(SAMPLE_SIZE),\n",
    "                        id_vars=CARDINAL_VARS,\n",
    "                        value_vars=Y_VARS,\n",
    "                        var_name=\"day_id\",\n",
    "                        value_name='demand')\n",
    "    else:\n",
    "        sales = pd.melt(sales,\n",
    "                        id_vars=CARDINAL_VARS,\n",
    "                        value_vars=Y_VARS,\n",
    "                        var_name=\"day_id\",\n",
    "                        value_name='demand')\n",
    "    print(\"[INFO] \", \"Melting  -- COMPLETE\", ctime(time()))\n",
    "\n",
    "    gc.collect()\n",
    "    \n",
    "    sales = pd.merge(sales, calendar, how=\"left\", left_on=\"day_id\", right_on=\"d\")\n",
    "    print(\"[INFO] \", \"Merging1 -- COMPLETE\", ctime(time()))\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    sales = pd.merge(sales, sell_prices, how=\"left\", on=[\"wm_yr_wk\", \"store_id\", \"item_id\"])\n",
    "    print(\"[INFO] \", \"Merging2 -- COMPLETE\", ctime(time()))\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    # if sell_price is NA\n",
    "    sales = sales.sort_values(by=CARDINAL_VARS+[\"date\"])\n",
    "    print(\"[INFO] \", \"Sorting -- COMPLETE\", ctime(time()))\n",
    "    \n",
    "    # remove till first non-zero demand per series\n",
    "    before = sales.shape\n",
    "    sales['cumul_demand'] = sales.groupby(['id'])['demand'].cumsum()\n",
    "    sales = sales.loc[sales['cumul_demand']>0].copy().reset_index(drop=True)\n",
    "    after = sales.shape\n",
    "    print(\"[INFO] \", \"Removal -- COMPLETE\", ctime(time()))\n",
    "    print(\"[INFO] \", \"Removal\", before, \"--->\", after)\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    sales[\"sell_price_available\"] = np.where(sales.sell_price.isna(), \"N\", \"Y\")\n",
    "    sales[[\"sell_price\"]] = sales.groupby([\"item_id\"])[[\"sell_price\"]].ffill()\n",
    "    sales[[\"sell_price\"]] = sales.groupby([\"item_id\"])[[\"sell_price\"]].bfill()\n",
    "    sales[\"sell_price\"] = sales[\"sell_price\"].fillna(3.5) # median price\n",
    "    sales = sales.drop([\"d\", \"wday\", 'date', 'wm_yr_wk', 'weekday', 'month', 'year'], axis=1)\n",
    "    \n",
    "    sales['day_id'] = sales['day_id'].astype(str).apply(lambda x: x[2:]).astype(np.int16)\n",
    "    sales[['state_id', 'store_id', 'cat_id', 'dept_id', 'item_id']] = sales[['state_id', 'store_id', 'cat_id', 'dept_id', 'item_id']].fillna(\"All\")\n",
    "    print(\"[INFO] \", \"Imputing -- COMPLETE\", ctime(time()))\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    sales['weights'] = sales[\"sell_price\"] * sales[\"demand\"]\n",
    "    sales['rolling_weights'] = sales.groupby(CARDINAL_VARS)['weights'].rolling(window=NUM_TIMESTEPS,\n",
    "                                                                               min_periods=1).sum().reset_index(drop=True)\n",
    "    \n",
    "    sales['weights'] = sales['weights'].apply(lambda x: np.max((1.0, x)))\n",
    "    sales['rolling_weights'] = sales['rolling_weights'].apply(lambda x: np.max((1.0, x)))\n",
    "    \n",
    "    sales['weights'] = sales['weights'].fillna(sales['weights'].max())\n",
    "    sales['rolling_weights'] = sales['rolling_weights'].fillna(sales['rolling_weights'].max())\n",
    "    \n",
    "    print(\"[INFO] \", \"Weighting -- COMPLETE\", ctime(time()))    \n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    print(\"[INFO] \", \"Final dataset contains\", sales.shape)\n",
    "    \n",
    "    return sales.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files...\n",
      "Mem. usage decreased to  0.12 Mb (41.9% reduction)\n",
      "Calendar has 1969 rows and 14 columns\n",
      "Mem. usage decreased to 130.48 Mb (37.5% reduction)\n",
      "Sell prices has 6841121 rows and 4 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3254: DtypeWarning: Columns (0,1,2,3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\pandas\\core\\indexing.py:671: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sales train validation has 42840 rows and 1947 columns\n",
      "Sales train validation has 42840 rows and 1975 columns\n",
      "{'calendar.shape': (1969, 14),\n",
      " 'sales.shape': (42840, 1975),\n",
      " 'sell_prices.shape': (6841121, 4),\n",
      " 'submission.shape': (60980, 29)}\n",
      "Mem. usage decreased to  0.15 Mb (30.4% reduction)\n",
      "[INFO]  Starting  -- COMPLETE Sun Jun 28 04:48:49 2020\n",
      "[INFO]  Melting  -- COMPLETE Sun Jun 28 04:48:50 2020\n",
      "[INFO]  Merging1 -- COMPLETE Sun Jun 28 04:48:50 2020\n",
      "[INFO]  Merging2 -- COMPLETE Sun Jun 28 04:48:54 2020\n",
      "[INFO]  Sorting -- COMPLETE Sun Jun 28 04:48:57 2020\n",
      "[INFO]  Removal -- COMPLETE Sun Jun 28 04:48:57 2020\n",
      "[INFO]  Removal (984500, 30) ---> (794276, 31)\n",
      "[INFO]  Imputing -- COMPLETE Sun Jun 28 04:48:59 2020\n",
      "[INFO]  Weighting -- COMPLETE Sun Jun 28 04:49:14 2020\n",
      "[INFO]  Final dataset contains (794276, 27)\n",
      "Saving all data to --->  ../data/preprocessed/\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(PICKLE_DIR+\"merged_df.pickle\"):\n",
    "    \n",
    "    calendar, sell_prices, sales, submission = read_data()\n",
    "    \n",
    "    calendar = add_dateparts(calendar, \"date\")\n",
    "    \n",
    "    Y_VARS = sales.columns[sales.columns.str.startswith(\"d_\")]\n",
    "\n",
    "    data = melt_join_fill(sales, calendar, sell_prices)\n",
    "    \n",
    "    del sales, calendar, sell_prices\n",
    "    gc.collect()\n",
    "\n",
    "    data[\"demand_lag\"] = data.groupby('id')['demand'].apply(lambda x: diff_w_first(x,60)).reset_index(drop=False)['demand']\n",
    "    data = add_features(df=data, grouping_col=\"id\", col=\"demand_lag\", roll_sizes=[7, 30, 90, 365], lag_sizes=[7, 30, 90, 365])\n",
    "\n",
    "    print(\"Saving all data to ---> \", PICKLE_DIR)\n",
    "    with open(os.path.join(PICKLE_DIR, \"merged_df.pickle\"),\"wb\") as f:\n",
    "        pickle.dump((data), f)\n",
    "else:\n",
    "    print(\"Pickle exists hence loading from pickle file ---> \", PICKLE_DIR)\n",
    "    with open(os.path.join(PICKLE_DIR+\"merged_df.pickle\"), \"rb\") as f:\n",
    "        data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['state_id', 'store_id', 'cat_id', 'dept_id', 'item_id', 'id', 'day_id',\n",
      "       'demand', 'event_name_1', 'event_type_1', 'event_name_2',\n",
      "       'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI', 'Date', 'Year',\n",
      "       'Quarter', 'Month', 'Week', 'Day', 'DOW', 'sell_price', 'cumul_demand',\n",
      "       'sell_price_available', 'weights', 'rolling_weights', 'demand_lag',\n",
      "       'demand_lag_mean_s', 'demand_lag_std_s', 'demand_lag_min_s',\n",
      "       'demand_lag_max_s', 'demand_lag_scaled_s', 'demand_lag_mean_rs_7',\n",
      "       'demand_lag_min_rs_7', 'demand_lag_max_rs_7', 'demand_lag_std_rs_7',\n",
      "       'demand_lag_mean_rs_30', 'demand_lag_min_rs_30', 'demand_lag_max_rs_30',\n",
      "       'demand_lag_std_rs_30', 'demand_lag_mean_rs_90', 'demand_lag_min_rs_90',\n",
      "       'demand_lag_max_rs_90', 'demand_lag_std_rs_90',\n",
      "       'demand_lag_mean_rs_365', 'demand_lag_min_rs_365',\n",
      "       'demand_lag_max_rs_365', 'demand_lag_std_rs_365', 'demand_lag_lag_7',\n",
      "       'demand_lag_lag_30', 'demand_lag_lag_90', 'demand_lag_lag_365'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1885, 1913, 1941, 1969)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1913-28, 1913, 1913 + 28, 1913 + 28 + 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Timestamp('2011-01-29 00:00:00'), Timestamp('2016-06-19 00:00:00'))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Date.min(), data.Date.max()#, data.Date.max() + 28, data.Date.max() + 28 + 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(794276, 53)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>id</th>\n",
       "      <th>day_id</th>\n",
       "      <th>demand</th>\n",
       "      <th>event_name_1</th>\n",
       "      <th>event_type_1</th>\n",
       "      <th>...</th>\n",
       "      <th>demand_lag_max_rs_90</th>\n",
       "      <th>demand_lag_std_rs_90</th>\n",
       "      <th>demand_lag_mean_rs_365</th>\n",
       "      <th>demand_lag_min_rs_365</th>\n",
       "      <th>demand_lag_max_rs_365</th>\n",
       "      <th>demand_lag_std_rs_365</th>\n",
       "      <th>demand_lag_lag_7</th>\n",
       "      <th>demand_lag_lag_30</th>\n",
       "      <th>demand_lag_lag_90</th>\n",
       "      <th>demand_lag_lag_365</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CA</td>\n",
       "      <td>All</td>\n",
       "      <td>All</td>\n",
       "      <td>All</td>\n",
       "      <td>FOODS_1_001</td>\n",
       "      <td>CA_FOODS_1_001</td>\n",
       "      <td>1</td>\n",
       "      <td>6.00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>538.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>538.00</td>\n",
       "      <td>538.00</td>\n",
       "      <td>538.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>6.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CA</td>\n",
       "      <td>All</td>\n",
       "      <td>All</td>\n",
       "      <td>All</td>\n",
       "      <td>FOODS_1_001</td>\n",
       "      <td>CA_FOODS_1_001</td>\n",
       "      <td>2</td>\n",
       "      <td>3.00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>538.00</td>\n",
       "      <td>99.70</td>\n",
       "      <td>467.50</td>\n",
       "      <td>397.00</td>\n",
       "      <td>538.00</td>\n",
       "      <td>99.70</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CA</td>\n",
       "      <td>All</td>\n",
       "      <td>All</td>\n",
       "      <td>All</td>\n",
       "      <td>FOODS_1_001</td>\n",
       "      <td>CA_FOODS_1_001</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>538.00</td>\n",
       "      <td>90.94</td>\n",
       "      <td>434.33</td>\n",
       "      <td>368.00</td>\n",
       "      <td>538.00</td>\n",
       "      <td>90.94</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CA</td>\n",
       "      <td>All</td>\n",
       "      <td>All</td>\n",
       "      <td>All</td>\n",
       "      <td>FOODS_1_001</td>\n",
       "      <td>CA_FOODS_1_001</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>538.00</td>\n",
       "      <td>85.39</td>\n",
       "      <td>413.25</td>\n",
       "      <td>350.00</td>\n",
       "      <td>538.00</td>\n",
       "      <td>85.39</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CA</td>\n",
       "      <td>All</td>\n",
       "      <td>All</td>\n",
       "      <td>All</td>\n",
       "      <td>FOODS_1_001</td>\n",
       "      <td>CA_FOODS_1_001</td>\n",
       "      <td>5</td>\n",
       "      <td>7.00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>538.00</td>\n",
       "      <td>90.65</td>\n",
       "      <td>389.80</td>\n",
       "      <td>296.00</td>\n",
       "      <td>538.00</td>\n",
       "      <td>90.65</td>\n",
       "      <td>7.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>7.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  state_id store_id cat_id dept_id      item_id              id  day_id  \\\n",
       "0       CA      All    All     All  FOODS_1_001  CA_FOODS_1_001       1   \n",
       "1       CA      All    All     All  FOODS_1_001  CA_FOODS_1_001       2   \n",
       "2       CA      All    All     All  FOODS_1_001  CA_FOODS_1_001       3   \n",
       "3       CA      All    All     All  FOODS_1_001  CA_FOODS_1_001       4   \n",
       "4       CA      All    All     All  FOODS_1_001  CA_FOODS_1_001       5   \n",
       "\n",
       "   demand event_name_1 event_type_1  ... demand_lag_max_rs_90  \\\n",
       "0    6.00         None         None  ...               538.00   \n",
       "1    3.00         None         None  ...               538.00   \n",
       "2    2.00         None         None  ...               538.00   \n",
       "3    3.00         None         None  ...               538.00   \n",
       "4    7.00         None         None  ...               538.00   \n",
       "\n",
       "  demand_lag_std_rs_90  demand_lag_mean_rs_365  demand_lag_min_rs_365  \\\n",
       "0                 0.00                  538.00                 538.00   \n",
       "1                99.70                  467.50                 397.00   \n",
       "2                90.94                  434.33                 368.00   \n",
       "3                85.39                  413.25                 350.00   \n",
       "4                90.65                  389.80                 296.00   \n",
       "\n",
       "   demand_lag_max_rs_365 demand_lag_std_rs_365  demand_lag_lag_7  \\\n",
       "0                 538.00                  0.00              6.00   \n",
       "1                 538.00                 99.70              3.00   \n",
       "2                 538.00                 90.94              2.00   \n",
       "3                 538.00                 85.39              3.00   \n",
       "4                 538.00                 90.65              7.00   \n",
       "\n",
       "   demand_lag_lag_30  demand_lag_lag_90  demand_lag_lag_365  \n",
       "0               6.00               6.00                6.00  \n",
       "1               3.00               3.00                3.00  \n",
       "2               2.00               2.00                2.00  \n",
       "3               3.00               3.00                3.00  \n",
       "4               7.00               7.00                7.00  \n",
       "\n",
       "[5 rows x 53 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data.shape); data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONT_VARS = ['snap_CA', 'snap_TX', 'snap_WI', 'sell_price',\n",
    "             'demand_lag', 'demand_lag_mean_s', 'demand_lag_std_s', 'demand_lag_min_s',\n",
    "             'demand_lag_max_s', 'demand_lag_scaled_s', 'demand_lag_mean_rs_7',\n",
    "             'demand_lag_min_rs_7', 'demand_lag_max_rs_7', 'demand_lag_std_rs_7',\n",
    "             'demand_lag_mean_rs_30', 'demand_lag_min_rs_30', 'demand_lag_max_rs_30',\n",
    "             'demand_lag_std_rs_30', 'demand_lag_mean_rs_90', 'demand_lag_min_rs_90',\n",
    "             'demand_lag_max_rs_90', 'demand_lag_std_rs_90', 'demand_lag_mean_rs_365',\n",
    "             'demand_lag_min_rs_365', 'demand_lag_max_rs_365', 'demand_lag_std_rs_365',\n",
    "             'demand_lag_lag_7', 'demand_lag_lag_30', 'demand_lag_lag_90', 'demand_lag_lag_365']\n",
    "CAT_VARS = ['state_id', 'store_id', 'cat_id', 'dept_id', 'item_id', 'event_name_1',\n",
    "            'event_type_1', 'event_name_2', 'event_type_2', 'Year', 'Quarter',\n",
    "            'Month', 'Week', 'Day', 'DOW', 'sell_price_available']\n",
    "DEP_VAR = ['demand']\n",
    "WEIGHT_VAR = ['weights', 'rolling_weights']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Category_Mapping = {}\n",
    "for c in CAT_VARS:\n",
    "    # convert columns to categories\n",
    "    data[c+\"_cat\"] = data[c].astype(\"category\")\n",
    "    \n",
    "    # save the mapping for later use\n",
    "    Category_Mapping.update({c+\"_cat\" : dict(enumerate(data[c+\"_cat\"].cat.categories))})\n",
    "    \n",
    "    # Copy categories as integer codes\n",
    "    data[c+\"_cat\"] = data[c+\"_cat\"].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DOW_cat': {'emb_sz': 3, 'max': 6, 'min': 0, 'nuniq': 7},\n",
      " 'Day_cat': {'emb_sz': 10, 'max': 30, 'min': 0, 'nuniq': 31},\n",
      " 'Month_cat': {'emb_sz': 6, 'max': 11, 'min': 0, 'nuniq': 12},\n",
      " 'Quarter_cat': {'emb_sz': 2, 'max': 3, 'min': 0, 'nuniq': 4},\n",
      " 'Week_cat': {'emb_sz': 10, 'max': 52, 'min': 0, 'nuniq': 53},\n",
      " 'Year_cat': {'emb_sz': 3, 'max': 5, 'min': 0, 'nuniq': 6},\n",
      " 'cat_id_cat': {'emb_sz': 2, 'max': 1, 'min': 0, 'nuniq': 2},\n",
      " 'dept_id_cat': {'emb_sz': 2, 'max': 0, 'min': 0, 'nuniq': 1},\n",
      " 'event_name_1_cat': {'emb_sz': 10, 'max': 30, 'min': 0, 'nuniq': 31},\n",
      " 'event_name_2_cat': {'emb_sz': 2, 'max': 4, 'min': 0, 'nuniq': 5},\n",
      " 'event_type_1_cat': {'emb_sz': 2, 'max': 4, 'min': 0, 'nuniq': 5},\n",
      " 'event_type_2_cat': {'emb_sz': 2, 'max': 2, 'min': 0, 'nuniq': 3},\n",
      " 'item_id_cat': {'emb_sz': 10, 'max': 461, 'min': 0, 'nuniq': 462},\n",
      " 'sell_price_available_cat': {'emb_sz': 2, 'max': 1, 'min': 0, 'nuniq': 2},\n",
      " 'state_id_cat': {'emb_sz': 2, 'max': 3, 'min': 0, 'nuniq': 4},\n",
      " 'store_id_cat': {'emb_sz': 5, 'max': 10, 'min': 0, 'nuniq': 11}}\n"
     ]
    }
   ],
   "source": [
    "Unique_Dict ={}\n",
    "for c in CAT_VARS:\n",
    "    col = c+\"_cat\"\n",
    "    Unique_Dict.update({col:{'min':data[col].min(), \n",
    "                             'max':data[col].max(), \n",
    "                             'nuniq':data[col].nunique(), \n",
    "                             'emb_sz':max(min(int(data[col].nunique() / 2), 10), 2)}})\n",
    "pprint(Unique_Dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_CAT_VARS = [col+\"_cat\" for col in CAT_VARS]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SCALING:\n",
    "    MMS = MinMaxScaler()\n",
    "    data[CONT_VARS] = MMS.fit_transform(data[CONT_VARS])\n",
    "\n",
    "    MMS_Y = MinMaxScaler(feature_range=(0, 0.8))\n",
    "    data[DEP_VAR] = MMS_Y.fit_transform(data[DEP_VAR])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['set'] = np.where(data.day_id.isin(TRAIN_INDICES), \"Train\",\n",
    "                       np.where(data.day_id.isin(VALID_INDICES), \"Valid\",\n",
    "                                np.where(data.day_id.isin(PUBLIC_INDICES), \"Public\", \"Private\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th colspan=\"2\" halign=\"left\">day_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>amin</th>\n",
       "      <th>amax</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>set</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Private</th>\n",
       "      <td>14000</td>\n",
       "      <td>1942</td>\n",
       "      <td>1969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Public</th>\n",
       "      <td>14000</td>\n",
       "      <td>1914</td>\n",
       "      <td>1941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train</th>\n",
       "      <td>752276</td>\n",
       "      <td>1</td>\n",
       "      <td>1885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Valid</th>\n",
       "      <td>14000</td>\n",
       "      <td>1886</td>\n",
       "      <td>1913</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id day_id      \n",
       "          count   amin  amax\n",
       "set                         \n",
       "Private   14000   1942  1969\n",
       "Public    14000   1914  1941\n",
       "Train    752276      1  1885\n",
       "Valid     14000   1886  1913"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('set').agg({\n",
    "    'id':'count',\n",
    "    'day_id':[np.min, np.max]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(752276, 44) (14000, 44) (14000, 44) (14000, 44)\n"
     ]
    }
   ],
   "source": [
    "train_data = data.loc[data.day_id.isin(TRAIN_INDICES)]\n",
    "valid_data = data.loc[data.day_id.isin(VALID_INDICES)]\n",
    "public_data = data.loc[data.day_id.isin(PUBLIC_INDICES)]\n",
    "private_data = data.loc[data.day_id.isin(PRIVATE_INDICES)]\n",
    "\n",
    "print(train_data.shape, valid_data.shape, public_data.shape, private_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape[0] == train_data.shape[0] + valid_data.shape[0] + private_data.shape[0] + private_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jun 28 03:18:28 2020\n"
     ]
    }
   ],
   "source": [
    "model_time = time()\n",
    "model_t = datetime.now()\n",
    "print(ctime(model_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    layers = []\n",
    "    inputs = []\n",
    "    for i,col in enumerate(CAT_VARS):\n",
    "        input_ = Input(shape=1, name=col+\"_cat\")\n",
    "        embedding =  Embedding(Unique_Dict[col+\"_cat\"]['nuniq'],\n",
    "                               Unique_Dict[col+\"_cat\"]['emb_sz'],\n",
    "                               name='emb_'+col)(input_)\n",
    "        vec = Flatten()(embedding)\n",
    "        layers.append(vec)\n",
    "        inputs.append(input_)\n",
    "    \n",
    "    for i, col in enumerate(CONT_VARS):\n",
    "        input_ = Input(shape=1, name=col)\n",
    "        layers.append(input_)\n",
    "        inputs.append(input_)\n",
    "    \n",
    "    concat_layer = concatenate(layers)\n",
    "    x = Dense(2048)(concat_layer)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(DROPOUT)(x, training=MC_DROPOUT)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Dense(1024)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(DROPOUT)(x, training=MC_DROPOUT)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Dense(512)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(DROPOUT)(x, training=MC_DROPOUT)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    x = Dense(256)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(DROPOUT)(x, training=MC_DROPOUT)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    x = Dense(128)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(DROPOUT)(x, training=MC_DROPOUT)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    x = Dense(64)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(DROPOUT)(x, training=MC_DROPOUT)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    x = Dense(32)(x)\n",
    "    x = Dropout(DROPOUT)(x, training=MC_DROPOUT)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    if SCALING:\n",
    "        demand = Dense(1, activation='sigmoid', name='demand')(x)\n",
    "    else:\n",
    "        demand = Dense(1, activation='relu', name='demand')(x)\n",
    "    \n",
    "    model = Model(inputs, demand)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "state_id_cat (InputLayer)       [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "store_id_cat (InputLayer)       [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cat_id_cat (InputLayer)         [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dept_id_cat (InputLayer)        [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "item_id_cat (InputLayer)        [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "event_name_1_cat (InputLayer)   [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "event_type_1_cat (InputLayer)   [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "event_name_2_cat (InputLayer)   [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "event_type_2_cat (InputLayer)   [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Year_cat (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Quarter_cat (InputLayer)        [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Month_cat (InputLayer)          [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Week_cat (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Day_cat (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "DOW_cat (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sell_price_available_cat (Input [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "emb_state_id (Embedding)        (None, 1, 2)         8           state_id_cat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "emb_store_id (Embedding)        (None, 1, 5)         55          store_id_cat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "emb_cat_id (Embedding)          (None, 1, 2)         4           cat_id_cat[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "emb_dept_id (Embedding)         (None, 1, 2)         2           dept_id_cat[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "emb_item_id (Embedding)         (None, 1, 10)        4620        item_id_cat[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "emb_event_name_1 (Embedding)    (None, 1, 10)        310         event_name_1_cat[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "emb_event_type_1 (Embedding)    (None, 1, 2)         10          event_type_1_cat[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "emb_event_name_2 (Embedding)    (None, 1, 2)         10          event_name_2_cat[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "emb_event_type_2 (Embedding)    (None, 1, 2)         6           event_type_2_cat[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "emb_Year (Embedding)            (None, 1, 3)         18          Year_cat[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "emb_Quarter (Embedding)         (None, 1, 2)         8           Quarter_cat[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "emb_Month (Embedding)           (None, 1, 6)         72          Month_cat[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "emb_Week (Embedding)            (None, 1, 10)        530         Week_cat[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "emb_Day (Embedding)             (None, 1, 10)        310         Day_cat[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "emb_DOW (Embedding)             (None, 1, 3)         21          DOW_cat[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "emb_sell_price_available (Embed (None, 1, 2)         4           sell_price_available_cat[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_32 (Flatten)            (None, 2)            0           emb_state_id[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_33 (Flatten)            (None, 5)            0           emb_store_id[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_34 (Flatten)            (None, 2)            0           emb_cat_id[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_35 (Flatten)            (None, 2)            0           emb_dept_id[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_36 (Flatten)            (None, 10)           0           emb_item_id[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_37 (Flatten)            (None, 10)           0           emb_event_name_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_38 (Flatten)            (None, 2)            0           emb_event_type_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_39 (Flatten)            (None, 2)            0           emb_event_name_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_40 (Flatten)            (None, 2)            0           emb_event_type_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_41 (Flatten)            (None, 3)            0           emb_Year[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_42 (Flatten)            (None, 2)            0           emb_Quarter[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_43 (Flatten)            (None, 6)            0           emb_Month[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_44 (Flatten)            (None, 10)           0           emb_Week[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_45 (Flatten)            (None, 10)           0           emb_Day[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_46 (Flatten)            (None, 3)            0           emb_DOW[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_47 (Flatten)            (None, 2)            0           emb_sell_price_available[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "snap_CA (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "snap_TX (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "snap_WI (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sell_price (InputLayer)         [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 77)           0           flatten_32[0][0]                 \n",
      "                                                                 flatten_33[0][0]                 \n",
      "                                                                 flatten_34[0][0]                 \n",
      "                                                                 flatten_35[0][0]                 \n",
      "                                                                 flatten_36[0][0]                 \n",
      "                                                                 flatten_37[0][0]                 \n",
      "                                                                 flatten_38[0][0]                 \n",
      "                                                                 flatten_39[0][0]                 \n",
      "                                                                 flatten_40[0][0]                 \n",
      "                                                                 flatten_41[0][0]                 \n",
      "                                                                 flatten_42[0][0]                 \n",
      "                                                                 flatten_43[0][0]                 \n",
      "                                                                 flatten_44[0][0]                 \n",
      "                                                                 flatten_45[0][0]                 \n",
      "                                                                 flatten_46[0][0]                 \n",
      "                                                                 flatten_47[0][0]                 \n",
      "                                                                 snap_CA[0][0]                    \n",
      "                                                                 snap_TX[0][0]                    \n",
      "                                                                 snap_WI[0][0]                    \n",
      "                                                                 sell_price[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 2048)         159744      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 2048)         8192        dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 2048)         0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 2048)         0           dropout_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 1024)         2098176     activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 1024)         4096        dense_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 1024)         0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 1024)         0           dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 512)          524800      activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 512)          2048        dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 512)          0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 512)          0           dropout_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 256)          131328      activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 256)          1024        dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 256)          0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 256)          0           dropout_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 128)          32896       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 128)          512         dense_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 128)          0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 128)          0           dropout_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 64)           8256        activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 64)           256         dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 64)           0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 64)           0           dropout_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 32)           2080        activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 32)           0           dense_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 32)           0           dropout_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "demand (Dense)                  (None, 1)            33          activation_20[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 2,979,429\n",
      "Trainable params: 2,971,365\n",
      "Non-trainable params: 8,064\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tclr = TriangularCyclicalLearningRate(\n",
    "    initial_learning_rate=MIN_LR,\n",
    "    maximal_learning_rate=MAX_LR,\n",
    "    step_size=STEP_SIZE * len(train_data)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcp = ModelCheckpoint(filepath=RESULTS_DIR+\"BestCheckpoint_\"+MODEL_VERSION+\".h5\", monitor='val_loss',\n",
    "                      verbose=0, save_best_only=True, save_weights_only=False, save_freq='epoch')\n",
    "csvl = CSVLogger(filename=RESULTS_DIR+\"LossLogs_\"+MODEL_VERSION+\".csv\",\n",
    "                 separator=\",\", append=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "PBL = PinballLoss(tau=[0.005,0.025,0.165,0.25,0.5,0.75,0.835,0.975,0.995])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = Adam(learning_rate=MAX_LR)\n",
    "model.compile(loss= PBL if MC_DROPOUT else 'mse',\n",
    "              optimizer=adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train on 752276 samples, validate on 14000 samples\n",
      "Epoch 1/2\n",
      "752276/752276 [==============================] - 15s 20us/sample - loss: 10.2259 - val_loss: nan\n",
      "Epoch 2/2\n",
      " 12288/752276 [..............................] - ETA: 10s - loss: 9.0363"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py:1020: RuntimeWarning: invalid value encountered in less\n",
      "  if self.monitor_op(current, self.best):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210944/752276 [=======>......................] - ETA: 8s - loss: 9.9005WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'val_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    766\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 767\u001b[1;33m       \u001b[1;32myield\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    768\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    598\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 599\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    600\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mc:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-5d1e5dd2fc29>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m#                     sample_weight=train_data[WEIGHT_VAR].to_dict(orient='series')['rolling_weights'],\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m                     callbacks=[mcp, csvl])\n\u001b[0m",
      "\u001b[1;32mc:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mc:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    395\u001b[0m                       total_epochs=1)\n\u001b[0;32m    396\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[1;32m--> 397\u001b[1;33m                                  prefix='val_')\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m    128\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    769\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m         \u001b[1;31m# Epochs only apply to `fit`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 771\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    772\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m       \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   2044\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2045\u001b[0m     \u001b[0mrow_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOrderedDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'epoch'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2046\u001b[1;33m     \u001b[0mrow_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2047\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2048\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\deepak\\miniconda3\\envs\\dev\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   2044\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2045\u001b[0m     \u001b[0mrow_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOrderedDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'epoch'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2046\u001b[1;33m     \u001b[0mrow_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2047\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2048\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'val_loss'"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=train_data[NEW_CAT_VARS + CONT_VARS].to_dict(orient='series'),\n",
    "                    y=train_data[DEP_VAR].to_dict(orient='series'),\n",
    "                    validation_data=(valid_data[NEW_CAT_VARS + CONT_VARS].to_dict(orient='series'),\n",
    "                                     valid_data[DEP_VAR].to_dict(orient='series'),\n",
    "                                     valid_data[WEIGHT_VAR].to_dict(orient='series')['rolling_weights']\n",
    "                                    ),\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=NUM_EPOCHS[0],\n",
    "                    shuffle=True,\n",
    "                    verbose=1,\n",
    "                    sample_weight=train_data[WEIGHT_VAR].to_dict(orient='series')['rolling_weights'],\n",
    "                    callbacks=[mcp, csvl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_names = ['loss']\n",
    "\n",
    "for i, j in zip(metric_names, ['val_'+i for i in metric_names]):\n",
    "    plt.plot(history.history[i])\n",
    "    plt.plot(history.history[j])\n",
    "    plt.title('Model '+i)\n",
    "    plt.ylabel(i.upper())\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(filepath=RESULTS_DIR+\"BestCheckpoint_\"+MODEL_VERSION+\".h5\", compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = Adam(learning_rate=tclr)\n",
    "model.compile(loss=PBL if MC_DROPOUT else 'mse',\n",
    "              optimizer=adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(x=train_data[NEW_CAT_VARS + CONT_VARS].to_dict(orient='series'),\n",
    "                    y=train_data[DEP_VAR].to_dict(orient='series'),\n",
    "                    validation_data=(valid_data[NEW_CAT_VARS + CONT_VARS].to_dict(orient='series'),\n",
    "                                     valid_data[DEP_VAR].to_dict(orient='series'),\n",
    "                                     valid_data[WEIGHT_VAR].to_dict(orient='series')['rolling_weights']\n",
    "                                    ),\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=NUM_EPOCHS[1],\n",
    "                    shuffle=True,\n",
    "                    verbose=1, \n",
    "                    sample_weight=train_data[WEIGHT_VAR].to_dict(orient='series')['rolling_weights'],\n",
    "                    callbacks=[mcp, csvl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_names = ['loss']\n",
    "\n",
    "for i, j in zip(metric_names, ['val_'+i for i in metric_names]):\n",
    "    plt.plot(history.history[i])\n",
    "    plt.plot(history.history[j])\n",
    "    plt.title('Model ' + i)\n",
    "    plt.ylabel(i.upper())\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(filepath=RESULTS_DIR+\"BestCheckpoint_\"+MODEL_VERSION+\".h5\", compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = Adam(learning_rate=MIN_LR*0.1)\n",
    "model.compile(loss=PBL if MC_DROPOUT else 'mse',\n",
    "              optimizer=adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x=train_data[NEW_CAT_VARS + CONT_VARS].to_dict(orient='series'),\n",
    "                    y=train_data[DEP_VAR].to_dict(orient='series'),\n",
    "                    validation_data=(valid_data[NEW_CAT_VARS + CONT_VARS].to_dict(orient='series'),\n",
    "                                     valid_data[DEP_VAR].to_dict(orient='series'),\n",
    "                                     valid_data[WEIGHT_VAR].to_dict(orient='series')['rolling_weights']\n",
    "                                    ),\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=NUM_EPOCHS[2],\n",
    "                    shuffle=True,\n",
    "                    verbose=1, \n",
    "                    sample_weight=train_data[WEIGHT_VAR].to_dict(orient='series')['rolling_weights'],\n",
    "                    callbacks=[mcp, csvl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metric_names = ['loss']\n",
    "\n",
    "for i, j in zip(metric_names, ['val_'+i for i in metric_names]):\n",
    "    plt.plot(history.history[i])\n",
    "    plt.plot(history.history[j])\n",
    "    plt.title('Model '+i)\n",
    "    plt.ylabel(i.upper())\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(filepath=RESULTS_DIR+\"FinalCheckpoint_\"+MODEL_VERSION+\".h5\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(RESULTS_DIR+\"BestCheckpoint_\"+MODEL_VERSION+\".h5\"):\n",
    "    model = tf.keras.models.load_model(filepath=RESULTS_DIR+\"BestCheckpoint_\"+MODEL_VERSION+\".h5\", compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(MC_SAMPLES):\n",
    "    if i == 0:\n",
    "        train_preds = model.predict(train_data[NEW_CAT_VARS + CONT_VARS].to_dict(orient='series'),\n",
    "                                    batch_size=PREDICT_BATCH_SIZE, verbose=0)\n",
    "\n",
    "        valid_preds = model.predict(valid_data[NEW_CAT_VARS + CONT_VARS].to_dict(orient='series'),\n",
    "                                    batch_size=PREDICT_BATCH_SIZE, verbose=0)\n",
    "\n",
    "        public_preds = model.predict(public_data[NEW_CAT_VARS + CONT_VARS].to_dict(orient='series'),\n",
    "                                     batch_size=PREDICT_BATCH_SIZE, verbose=0)\n",
    "\n",
    "        private_preds = model.predict(private_data[NEW_CAT_VARS + CONT_VARS].to_dict(orient='series'),\n",
    "                                      batch_size=PREDICT_BATCH_SIZE, verbose=0)\n",
    "        \n",
    "    else:\n",
    "        train_preds = np.concatenate((train_preds, \n",
    "                                      model.predict(train_data[NEW_CAT_VARS + CONT_VARS].to_dict(orient='series'),\n",
    "                                                    batch_size=PREDICT_BATCH_SIZE, verbose=0)), axis=1)\n",
    "\n",
    "        valid_preds = np.concatenate((valid_preds, \n",
    "                                      model.predict(valid_data[NEW_CAT_VARS + CONT_VARS].to_dict(orient='series'),\n",
    "                                                    batch_size=PREDICT_BATCH_SIZE, verbose=0)), axis=1)\n",
    "\n",
    "        public_preds = np.concatenate((public_preds, \n",
    "                                       model.predict(public_data[NEW_CAT_VARS + CONT_VARS].to_dict(orient='series'),\n",
    "                                                     batch_size=PREDICT_BATCH_SIZE, verbose=0)), axis=1)\n",
    "\n",
    "        private_preds = np.concatenate((private_preds, \n",
    "                                        model.predict(private_data[NEW_CAT_VARS + CONT_VARS].to_dict(orient='series'),\n",
    "                                                      batch_size=PREDICT_BATCH_SIZE, verbose=0)), axis=1)\n",
    "\n",
    "print(train_preds.shape, valid_preds.shape, public_preds.shape, private_preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma, median = train_preds.mean(axis=1), train_preds.std(axis=1), np.median(train_preds, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds[train_preds<0].sum(), valid_preds[valid_preds<0].sum(), public_preds[public_preds<0].sum(), private_preds[private_preds<0].sum(), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_normals(preds, ex_index):\n",
    "    num_bins = 100\n",
    "    mu_, sigma_ = mu[ex_index], sigma[ex_index]\n",
    "\n",
    "    n, bins, patches = plt.hist(preds[ex_index], num_bins,  \n",
    "                                density = 1,  \n",
    "                                color ='green', \n",
    "                                alpha = 0.7) \n",
    "\n",
    "    y = ((1 / (np.sqrt(2 * np.pi) * sigma_)) * np.exp(-0.5 * (1 / sigma_ * (bins - mu_))**2))\n",
    "\n",
    "    plt.plot(bins, y, '--', color ='black') \n",
    "\n",
    "    plt.xlabel('X-Axis')\n",
    "    plt.ylabel('Y-Axis')\n",
    "\n",
    "    plt.title('Prediction Range Plot:'+str(ex_index), fontweight =\"bold\") \n",
    "\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_normals(train_preds, np.random.randint(low=0, high=train_preds.shape[0], size=1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_normals(train_preds, mu.argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_normals(train_preds, sigma.argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_normals(train_preds, median.argmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_pred_stats(preds, data):\n",
    "    data['pred_mean'], data['pred_std'], data['p_0.500'] = preds.mean(axis=1), preds.std(axis=1), np.median(preds, axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Qs = np.array([0.5, 0.67, 0.95, 0.99])\n",
    "min_cols = ['p_'+str(min_col) for min_col in np.round((1-Qs)/2,3)]\n",
    "max_cols = ['p_'+str(max_col) for max_col in np.round((1+Qs)/2,3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "def create_intervals(preds, data):\n",
    "\n",
    "    Qs = np.array([0.5, 0.67, 0.95, 0.99])\n",
    "    min_cols = ['p_0.250', 'p_0.165', 'p_0.025', 'p_0.005'] #['p_'+str(min_col) for min_col in np.round((1-Qs)/2,3)]\n",
    "    max_cols = ['p_0.750', 'p_0.835', 'p_0.975', 'p_0.995'] #['p_'+str(max_col) for max_col in np.round((1+Qs)/2,3)]\n",
    "    \n",
    "    mu, sigma, median = preds.mean(axis=1), preds.std(axis=1), np.median(preds, axis=1)\n",
    "    data['p_mean'] = mu\n",
    "    for i, min_col, max_col in zip(Qs, min_cols, max_cols):\n",
    "        min_values, max_values = norm.interval(alpha=i, loc=mu, scale=sigma)\n",
    "        \n",
    "        sigma_flag = sigma==0\n",
    "        min_values[sigma_flag], max_values[sigma_flag] = 0, 0\n",
    "        print(i, \"Altered\",sum(sigma_flag),\"rows due to zero sigma\")\n",
    "        \n",
    "        min_neg_flag = min_values<0; max_neg_flag = max_values<0; \n",
    "        min_values[min_neg_flag], max_values[max_neg_flag] = 0, 0\n",
    "        print(i, \"Altered\",sum(min_neg_flag), sum(max_neg_flag),\"rows due to less than zero intervals\")\n",
    "        \n",
    "        data[min_col], data[max_col] = min_values, max_values\n",
    "    \n",
    "    data[min_cols + max_cols] = data[min_cols + max_cols].fillna(0)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = add_pred_stats(preds=train_preds, data=train_data)\n",
    "train_data = create_intervals(preds=train_preds, data=train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = add_pred_stats(preds=valid_preds, data=valid_data)\n",
    "valid_data = create_intervals(preds=valid_preds, data=valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_data = add_pred_stats(preds=public_preds, data=public_data)\n",
    "public_data = create_intervals(preds=public_preds, data=public_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "private_data = add_pred_stats(preds=private_preds, data=private_data)\n",
    "private_data = create_intervals(preds=private_preds, data=private_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_cols = [i for i in private_data.columns if i.startswith(\"p_\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in p_cols:\n",
    "    data[p] = 0\n",
    "    data[p].loc[data.day_id.isin(TRAIN_INDICES)] = train_data[p]\n",
    "    data[p].loc[data.day_id.isin(VALID_INDICES)] = valid_data[p]\n",
    "    data[p].loc[data.day_id.isin(PUBLIC_INDICES)] = public_data[p]\n",
    "    data[p].loc[data.day_id.isin(PRIVATE_INDICES)] = private_data[p]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = data.loc[data['set'].isin(['Private', 'Public'])][['id',\n",
    "                                                                'day_id',\n",
    "                                                                'set'] + p_cols].copy().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['day_id2'] = 'F' + pd.Series(np.where(submission['day_id'] <= max(PUBLIC_INDICES), \n",
    "                                                 submission['day_id'] - min(PUBLIC_INDICES) + 1, \n",
    "                                                 submission['day_id'] - min(PRIVATE_INDICES) + 1).astype(int).astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['id'] = np.where(submission['set']==\"Public\",\n",
    "                            submission['id'].str.replace(\"evaluation\", \"validation\"), \n",
    "                            submission['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.groupby('set').agg({\n",
    "    'day_id':[np.min, np.max]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = submission.melt(id_vars=['id','day_id2','set'],\n",
    "    value_vars=p_cols,\n",
    "    var_name='alpha',\n",
    "    value_name='prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['alpha'] = submission['alpha'].str.split(\"_\").apply(lambda x: x[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_combine(x,y):\n",
    "    return '_'.join(x.split(\"_\")[:-1]+[y]+[x.split(\"_\")[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['id'] = submission.apply(lambda x: split_combine(x['id'], x['alpha']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_file = submission.pivot(\n",
    "    values='prediction',\n",
    "    index='id',\n",
    "    columns='day_id2').reset_index(drop=False)\n",
    "submission_file = submission_file[['id']+[\"F\"+str(i) for i in range(1,29,1)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_file.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_file.to_csv('../results/submission_'+MODEL_VERSION+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time()\n",
    "end = datetime.now()\n",
    "print(ctime(end_time))\n",
    "print(end - start, model_t - start, model_t - end)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
